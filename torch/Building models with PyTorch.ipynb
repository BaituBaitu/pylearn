{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f95dfe6",
   "metadata": {},
   "source": [
    "本页面练习pytorch模型。参考pytorch官方教程页面中的同名部分\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "\n",
    "## `torch.nn.Module` and `torch.nn.Parameter`\n",
    "一般我们定义一个新的神经网络时，继承自类`torch.nn.Module`,，其中所有模型参数是类`torch.nn.Parameter`的instance.These parameters may be accessed through the `parameters()` method on the Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39328214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T07:15:21.288075Z",
     "start_time": "2022-08-01T07:15:19.367085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0706,  0.0302,  0.0636,  ...,  0.0524, -0.0039,  0.0952],\n",
      "        [-0.0879,  0.0449, -0.0947,  ...,  0.0522, -0.0343,  0.0918],\n",
      "        [-0.0735,  0.0914, -0.0796,  ...,  0.0145,  0.0166, -0.0992],\n",
      "        ...,\n",
      "        [ 0.0501, -0.0721,  0.0627,  ..., -0.0322,  0.0625,  0.0443],\n",
      "        [ 0.0007,  0.0360,  0.0270,  ...,  0.0230,  0.0898, -0.0896],\n",
      "        [-0.0099,  0.0504,  0.0816,  ..., -0.0648,  0.0191,  0.0771]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0509,  0.0045,  0.0606, -0.0004,  0.0384,  0.0341, -0.0694, -0.0241,\n",
      "        -0.0970,  0.0419, -0.0061,  0.0358,  0.0389,  0.0709, -0.0919,  0.0003,\n",
      "         0.0051, -0.0936,  0.0576,  0.0645,  0.0902, -0.0322, -0.0305, -0.0568,\n",
      "        -0.0146,  0.0469,  0.0968,  0.0798,  0.0142, -0.0207, -0.0082,  0.0708,\n",
      "         0.0756,  0.0068, -0.0010, -0.0529, -0.0784,  0.0011, -0.0036, -0.0419,\n",
      "        -0.0003, -0.0984, -0.0861, -0.0891, -0.0767, -0.0791,  0.0479, -0.0294,\n",
      "        -0.0873,  0.0786,  0.0562, -0.0876,  0.0203, -0.0548,  0.0846,  0.0315,\n",
      "        -0.0937, -0.0736,  0.0096, -0.0713,  0.0043,  0.0110, -0.0670,  0.0527,\n",
      "        -0.0582,  0.0566, -0.0324,  0.0855, -0.0592, -0.0119, -0.0055, -0.0908,\n",
      "        -0.0423, -0.0906,  0.0320, -0.0747, -0.0626, -0.0537,  0.0471,  0.0208,\n",
      "        -0.0641, -0.0194,  0.0219,  0.0872,  0.0046, -0.0139,  0.0710, -0.0631,\n",
      "        -0.0327, -0.0880, -0.0744, -0.0176, -0.0412, -0.0961,  0.0853, -0.0893,\n",
      "         0.0594,  0.0097, -0.0322,  0.0055, -0.0386, -0.0196,  0.0297, -0.0097,\n",
      "        -0.0188, -0.0517,  0.0657,  0.0194, -0.0062, -0.0929,  0.0573, -0.0668,\n",
      "        -0.0240, -0.0925,  0.0379,  0.0865,  0.0114, -0.0196, -0.0978,  0.0424,\n",
      "         0.0367, -0.0578, -0.0976, -0.0032,  0.0700, -0.0186,  0.0963, -0.0119,\n",
      "         0.0131, -0.0983, -0.0458, -0.0593, -0.0698, -0.0455,  0.0397, -0.0363,\n",
      "         0.0718,  0.0844, -0.0807, -0.0138, -0.0539, -0.0250,  0.0641,  0.0714,\n",
      "        -0.0437, -0.0305, -0.0900,  0.0650,  0.0468,  0.0693, -0.0532,  0.0330,\n",
      "         0.0719, -0.0308, -0.0822, -0.0866,  0.0261, -0.0120,  0.0903,  0.0790,\n",
      "         0.0891,  0.0313,  0.0282, -0.0518,  0.0864, -0.0772,  0.0742,  0.0900,\n",
      "         0.0434, -0.0907, -0.0222,  0.0216, -0.0890,  0.0321, -0.0813, -0.0194,\n",
      "        -0.0322,  0.0579,  0.0880, -0.0810,  0.0680,  0.0231,  0.0228, -0.0033,\n",
      "        -0.0514, -0.0398, -0.0845, -0.0479,  0.0575,  0.0460,  0.0587,  0.1000,\n",
      "         0.0526,  0.0152,  0.0512,  0.0294, -0.0491, -0.0807, -0.0293, -0.0613],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0691,  0.0666, -0.0674,  ...,  0.0531, -0.0559, -0.0423],\n",
      "        [-0.0046,  0.0400,  0.0035,  ..., -0.0287, -0.0163,  0.0318],\n",
      "        [-0.0480, -0.0386,  0.0500,  ...,  0.0409, -0.0345, -0.0187],\n",
      "        ...,\n",
      "        [-0.0044, -0.0123,  0.0586,  ...,  0.0423,  0.0247,  0.0292],\n",
      "        [-0.0127, -0.0186,  0.0431,  ..., -0.0060, -0.0373,  0.0218],\n",
      "        [ 0.0186, -0.0589,  0.0149,  ...,  0.0619, -0.0206,  0.0179]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0690, -0.0589, -0.0438, -0.0248, -0.0418, -0.0028, -0.0593, -0.0583,\n",
      "        -0.0230, -0.0428], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0691,  0.0666, -0.0674,  ...,  0.0531, -0.0559, -0.0423],\n",
      "        [-0.0046,  0.0400,  0.0035,  ..., -0.0287, -0.0163,  0.0318],\n",
      "        [-0.0480, -0.0386,  0.0500,  ...,  0.0409, -0.0345, -0.0187],\n",
      "        ...,\n",
      "        [-0.0044, -0.0123,  0.0586,  ...,  0.0423,  0.0247,  0.0292],\n",
      "        [-0.0127, -0.0186,  0.0431,  ..., -0.0060, -0.0373,  0.0218],\n",
      "        [ 0.0186, -0.0589,  0.0149,  ...,  0.0619, -0.0206,  0.0179]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0690, -0.0589, -0.0438, -0.0248, -0.0418, -0.0028, -0.0593, -0.0583,\n",
      "        -0.0230, -0.0428], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61c9d9",
   "metadata": {},
   "source": [
    "## Common Layer Types\n",
    "### Linear Layers\n",
    "If a model has m inputs and n outputs, the weights will be an m x n matrix. For example:  \n",
    "\n",
    "注意在参数中有bias，即常量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55764f04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T07:24:03.747971Z",
     "start_time": "2022-08-01T07:24:03.736396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.6346, 0.6580, 0.4436]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.3030,  0.2797, -0.1077],\n",
      "        [ 0.3205,  0.2898, -0.3379]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3193, 0.3791], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[0.2632, 0.6232]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbef933",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "torch.nn.Conv2d(input_channel, output_channel, kernel_size),第一个参数是输入的channel数量，对于黑白图像取1，对于RGB三通道图像取3，第二个参数是输出的卷积特征的个数，第三个是卷积核大小。Here, the “5” means we’ve chosen a 5x5 kernel. (If you want a kernel with height different from width, you can specify a tuple for this argument - e.g., (3, 5) to get a 3x5 convolution kernel.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2a6b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T07:31:22.072448Z",
     "start_time": "2022-08-01T07:31:22.061907Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c800c",
   "metadata": {},
   "source": [
    "### Recurrent Layers\n",
    "下图展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步 $t$, 隐状态的计算可以被视为:\n",
    "1. 拼接当前时间步 $t$ 的输入 $\\mathbf{X}_{t}$ 和前一时间步 $t - 1$ 的隐状态 $\\mathbf{H}$ ；\n",
    "2. 将拼接的结果送入带有激活函数 $\\phi$ 的全连接层。全连接层的输出是当前时间步 $t$ 的隐状态 $\\mathbf{H}_{t}$ 。\n",
    "在本例中, 模型参数是 $\\mathbf{W}_{xh}$ 和 $\\mathbf{W}_{hh}$ 的拼接, 以及 $\\mathbf{b}_{h}$ 的偏置, 所有这些参数都来自 (8.4.5)。当前时间步 $t$ 的隐 状态 $\\mathbf{H}_{t}$ 将参与计算下一时间步 $t+1$ 的隐状态 $\\mathbf{H}_{t}$ 。而且 $\\mathbf{H}_{t}$ 还将送入全连接输出层, 用于计算当前时间步 $t$ 的输出 $\\mathbf{O}_{t}$ 。\n",
    "![rnn](https://zh.d2l.ai/_images/rnn.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f079135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T08:59:32.751211Z",
     "start_time": "2022-08-01T08:59:32.742784Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocabulary_size, tagset_size):\n",
    "        super(self, LSTMTagger).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(\n",
    "            vocabulary_size, embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        print(embeds)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))  \n",
    "        # input of shape (seq_len, batch, input_size)\n",
    "        # output of shape (seq_len, batch, num_directions * hidden_size):\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1))  \n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb964504",
   "metadata": {},
   "source": [
    "## maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe993888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T09:23:00.554265Z",
     "start_time": "2022-08-01T09:23:00.538951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5973, 0.2610, 0.5401, 0.3649, 0.4927, 0.2900],\n",
      "         [0.5783, 0.4137, 0.2434, 0.4632, 0.5726, 0.0423],\n",
      "         [0.1840, 0.3327, 0.1273, 0.8292, 0.5662, 0.3883],\n",
      "         [0.9159, 0.8889, 0.7790, 0.0520, 0.8544, 0.1143],\n",
      "         [0.8702, 0.8769, 0.1738, 0.8905, 0.8930, 0.5439],\n",
      "         [0.8401, 0.1985, 0.3133, 0.3256, 0.4264, 0.8857]]])\n",
      "tensor([[[0.5973, 0.8292],\n",
      "         [0.9159, 0.8930]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73546a6",
   "metadata": {},
   "source": [
    "## dropout layers\n",
    "Dropout layers are a tool for encouraging sparse representations in your model - that is, pushing it to do inference with less data.\n",
    "\n",
    "Dropout layers work by randomly setting parts of the input tensor during training - dropout layers are always turned off for inference. This forces the model to learn against this masked or reduced dataset. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1c6e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T09:26:58.920961Z",
     "start_time": "2022-08-01T09:26:58.911033Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3020, 0.7684, 0.0335, 0.0034],\n",
      "         [0.9903, 0.7513, 0.4531, 0.1564],\n",
      "         [0.7358, 0.1040, 0.3232, 0.5796],\n",
      "         [0.9785, 0.0677, 0.5032, 0.1122]]])\n",
      "tensor([[[0.5034, 1.2807, 0.0558, 0.0056],\n",
      "         [1.6505, 1.2521, 0.7552, 0.0000],\n",
      "         [1.2263, 0.1734, 0.5387, 0.0000],\n",
      "         [1.6309, 0.1129, 0.8387, 0.1870]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(my_tensor)\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f805f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
