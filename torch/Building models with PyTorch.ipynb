{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f95dfe6",
   "metadata": {},
   "source": [
    "本页面练习pytorch模型。参考pytorch官方教程页面中的同名部分\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html  \n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html \n",
    "\n",
    "## `torch.nn.Module` and `torch.nn.Parameter`\n",
    "一般我们定义一个新的神经网络时，继承自类`torch.nn.Module`,，其中所有模型参数是类`torch.nn.Parameter`的instance.These parameters may be accessed through the `parameters()` method on the Module class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39328214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T01:34:27.804580Z",
     "start_time": "2022-08-02T01:34:25.239638Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0665, -0.0317, -0.0161,  ...,  0.0247, -0.0110,  0.0676],\n",
      "        [ 0.0903,  0.0648, -0.0361,  ..., -0.0463,  0.0501,  0.0814],\n",
      "        [ 0.0472,  0.0279, -0.0602,  ..., -0.0508, -0.0065,  0.0562],\n",
      "        ...,\n",
      "        [ 0.0020,  0.0845, -0.0537,  ...,  0.0672,  0.0140,  0.0595],\n",
      "        [ 0.0692, -0.0873,  0.0907,  ..., -0.0228, -0.0061, -0.0294],\n",
      "        [ 0.0515, -0.0577,  0.0635,  ..., -0.0032, -0.0167,  0.0475]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0523,  0.0792, -0.0380,  0.0412, -0.0628, -0.0371, -0.0236,  0.0607,\n",
      "        -0.0789, -0.0279,  0.0750, -0.0321,  0.0977, -0.0294, -0.0993,  0.0419,\n",
      "         0.0723,  0.0818,  0.0931,  0.0574,  0.0219,  0.0449,  0.0906, -0.0095,\n",
      "         0.0052,  0.0897, -0.0697,  0.0382, -0.0056, -0.0222, -0.0235, -0.0533,\n",
      "         0.0814,  0.0560, -0.0279,  0.0318,  0.0853, -0.0084, -0.0140, -0.0199,\n",
      "         0.0265, -0.0602,  0.0539,  0.0882,  0.0423, -0.0157,  0.0037, -0.0454,\n",
      "        -0.0946, -0.0436,  0.0148,  0.0292, -0.0892,  0.0978, -0.0412,  0.0241,\n",
      "         0.0248,  0.0552, -0.0733,  0.0503,  0.0159, -0.0205,  0.0049,  0.0338,\n",
      "         0.0166,  0.0693,  0.0354, -0.0639, -0.0133,  0.0113, -0.0166, -0.0638,\n",
      "        -0.0933, -0.0162, -0.0964, -0.0974,  0.0057,  0.0947, -0.0804, -0.0989,\n",
      "        -0.0090,  0.0315,  0.0828, -0.0578,  0.0653,  0.0477,  0.0605,  0.0548,\n",
      "        -0.0758,  0.0776,  0.0693,  0.0675,  0.0402, -0.0571, -0.0217, -0.0886,\n",
      "         0.0265, -0.0561,  0.0546, -0.0447,  0.0145,  0.0313, -0.0244, -0.0665,\n",
      "        -0.0482,  0.0453,  0.0489, -0.0184, -0.0496, -0.0083,  0.0697,  0.0780,\n",
      "        -0.0688, -0.0979,  0.0893,  0.0910, -0.0552,  0.0877, -0.0650, -0.0389,\n",
      "         0.0090, -0.0142, -0.0478, -0.0218, -0.0759,  0.0852, -0.0513,  0.0203,\n",
      "         0.0738, -0.0519, -0.0851, -0.0198,  0.0952, -0.0812, -0.0142,  0.0239,\n",
      "         0.0708,  0.0388,  0.0144,  0.0258,  0.0824, -0.0550,  0.0636,  0.0897,\n",
      "        -0.0284,  0.0022, -0.0105, -0.0822, -0.0165,  0.0467, -0.0571, -0.0367,\n",
      "         0.0352, -0.0843, -0.0454, -0.0144,  0.0446, -0.0547,  0.0287, -0.0805,\n",
      "        -0.0384, -0.0435,  0.0380, -0.0976, -0.0867,  0.0144,  0.0974,  0.0893,\n",
      "         0.0417,  0.0936,  0.0360, -0.0251, -0.0629,  0.0668,  0.0008,  0.0346,\n",
      "         0.0690,  0.0280, -0.0852,  0.0980,  0.0691,  0.0409,  0.0827,  0.0205,\n",
      "         0.0766, -0.0033, -0.0974, -0.0497, -0.0379, -0.0337,  0.0211, -0.0572,\n",
      "         0.0539, -0.0983, -0.0421,  0.0992,  0.0080, -0.0243,  0.0475, -0.0862],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0334,  0.0290,  0.0452,  ..., -0.0135,  0.0593,  0.0291],\n",
      "        [-0.0502,  0.0483,  0.0176,  ..., -0.0246,  0.0014,  0.0692],\n",
      "        [ 0.0082,  0.0218,  0.0608,  ...,  0.0291, -0.0099,  0.0626],\n",
      "        ...,\n",
      "        [ 0.0132,  0.0627,  0.0674,  ..., -0.0616,  0.0034, -0.0020],\n",
      "        [ 0.0197,  0.0111, -0.0387,  ...,  0.0205,  0.0353, -0.0099],\n",
      "        [ 0.0015, -0.0446, -0.0469,  ...,  0.0411,  0.0089, -0.0086]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0042, -0.0102, -0.0360, -0.0514, -0.0628, -0.0392, -0.0113,  0.0628,\n",
      "         0.0357, -0.0627], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0334,  0.0290,  0.0452,  ..., -0.0135,  0.0593,  0.0291],\n",
      "        [-0.0502,  0.0483,  0.0176,  ..., -0.0246,  0.0014,  0.0692],\n",
      "        [ 0.0082,  0.0218,  0.0608,  ...,  0.0291, -0.0099,  0.0626],\n",
      "        ...,\n",
      "        [ 0.0132,  0.0627,  0.0674,  ..., -0.0616,  0.0034, -0.0020],\n",
      "        [ 0.0197,  0.0111, -0.0387,  ...,  0.0205,  0.0353, -0.0099],\n",
      "        [ 0.0015, -0.0446, -0.0469,  ...,  0.0411,  0.0089, -0.0086]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0042, -0.0102, -0.0360, -0.0514, -0.0628, -0.0392, -0.0113,  0.0628,\n",
      "         0.0357, -0.0627], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61c9d9",
   "metadata": {},
   "source": [
    "## Common Layer Types\n",
    "### Linear Layers\n",
    "If a model has m inputs and n outputs, the weights will be an m x n matrix. For example:  \n",
    "\n",
    "注意在参数中有bias，即常量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55764f04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T01:34:27.830650Z",
     "start_time": "2022-08-02T01:34:27.809254Z"
    },
    "code_folding": [
     6
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.0950, 0.4133, 0.2453]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.2651, -0.2092,  0.0635],\n",
      "        [ 0.0273, -0.2154,  0.3392]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3514, -0.0401], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[ 0.2553, -0.0434]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbef933",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "torch.nn.Conv2d(input_channel, output_channel, kernel_size),第一个参数是输入的channel数量，对于黑白图像取1，对于RGB三通道图像取3，第二个参数是输出的卷积特征的个数，第三个是卷积核大小。Here, the “5” means we’ve chosen a 5x5 kernel. (If you want a kernel with height different from width, you can specify a tuple for this argument - e.g., (3, 5) to get a 3x5 convolution kernel.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2a6b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T01:34:27.842819Z",
     "start_time": "2022-08-02T01:34:27.833971Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c800c",
   "metadata": {},
   "source": [
    "### Recurrent Layers\n",
    "下图展示了循环神经网络在三个相邻时间步的计算逻辑。在任意时间步 $t$, 隐状态的计算可以被视为:\n",
    "1. 拼接当前时间步 $t$ 的输入 $\\mathbf{X}_{t}$ 和前一时间步 $t - 1$ 的隐状态 $\\mathbf{H}$ ；\n",
    "2. 将拼接的结果送入带有激活函数 $\\phi$ 的全连接层。全连接层的输出是当前时间步 $t$ 的隐状态 $\\mathbf{H}_{t}$ 。\n",
    "在本例中, 模型参数是 $\\mathbf{W}_{xh}$ 和 $\\mathbf{W}_{hh}$ 的拼接, 以及 $\\mathbf{b}_{h}$ 的偏置, 所有这些参数都来自 (8.4.5)。当前时间步 $t$ 的隐 状态 $\\mathbf{H}_{t}$ 将参与计算下一时间步 $t+1$ 的隐状态 $\\mathbf{H}_{t}$ 。而且 $\\mathbf{H}_{t}$ 还将送入全连接输出层, 用于计算当前时间步 $t$ 的输出 $\\mathbf{O}_{t}$ 。\n",
    "![rnn](https://zh.d2l.ai/_images/rnn.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f079135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T01:34:27.861850Z",
     "start_time": "2022-08-02T01:34:27.854098Z"
    },
    "code_folding": [
     1,
     9
    ]
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocabulary_size, tagset_size):\n",
    "        super(self, LSTMTagger).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(\n",
    "            vocabulary_size, embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        print(embeds)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))  \n",
    "        # input of shape (seq_len, batch, input_size)\n",
    "        # output of shape (seq_len, batch, num_directions * hidden_size):\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1))  \n",
    "        tag_scores = F.log_softmax(tag_space,dim=1)\n",
    "        return tag_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb964504",
   "metadata": {},
   "source": [
    "## maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe993888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T01:34:27.877714Z",
     "start_time": "2022-08-02T01:34:27.865174Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9950, 0.1662, 0.9341, 0.5566, 0.4886, 0.1871],\n",
      "         [0.1016, 0.8775, 0.2024, 0.3564, 0.0677, 0.9847],\n",
      "         [0.5776, 0.6127, 0.0346, 0.2698, 0.3419, 0.5168],\n",
      "         [0.7070, 0.2477, 0.2281, 0.7483, 0.2758, 0.1943],\n",
      "         [0.6796, 0.4139, 0.2979, 0.0630, 0.5987, 0.2688],\n",
      "         [0.5326, 0.6068, 0.2797, 0.7058, 0.2514, 0.4582]]])\n",
      "tensor([[[0.9950, 0.9847],\n",
      "         [0.7070, 0.7483]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73546a6",
   "metadata": {},
   "source": [
    "## dropout layers\n",
    "Dropout layers are a tool for encouraging sparse representations in your model - that is, pushing it to do inference with less data.\n",
    "\n",
    "Dropout layers work by randomly setting parts of the input tensor during training - dropout layers are always turned off for inference. This forces the model to learn against this masked or reduced dataset. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1c6e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T01:34:27.898548Z",
     "start_time": "2022-08-02T01:34:27.880709Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8789, 0.3455, 0.1694, 0.9645],\n",
      "         [0.3926, 0.3636, 0.8117, 0.5433],\n",
      "         [0.6180, 0.2305, 0.5149, 0.3590],\n",
      "         [0.4198, 0.3174, 0.2045, 0.9115]]])\n",
      "tensor([[[1.4648, 0.0000, 0.2823, 1.6074],\n",
      "         [0.0000, 0.6059, 1.3528, 0.9054],\n",
      "         [1.0300, 0.0000, 0.8581, 0.0000],\n",
      "         [0.6997, 0.5291, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(my_tensor)\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad024fc1",
   "metadata": {},
   "source": [
    "## WHAT IS TORCH.NN REALLY\n",
    "接下来这一部分参考pytorch官方教程中的同名部分\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "使用MNIST数据集，每个图像是28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeba425f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T02:46:43.580445Z",
     "start_time": "2022-08-02T02:46:42.538567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "        \n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "        \n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)   \n",
    "\n",
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae0588",
   "metadata": {},
   "source": [
    "### No torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec32680d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T02:54:44.181217Z",
     "start_time": "2022-08-02T02:54:44.168012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: \n",
      " tensor([[-0.0168,  0.0169, -0.0105,  ...,  0.0385,  0.0337,  0.0583],\n",
      "        [ 0.0036, -0.0311, -0.0153,  ..., -0.0311, -0.0082,  0.0361],\n",
      "        [ 0.0639, -0.0398, -0.0277,  ...,  0.0520, -0.0176, -0.0053],\n",
      "        ...,\n",
      "        [-0.0447,  0.0192, -0.0362,  ..., -0.0046,  0.0104,  0.0075],\n",
      "        [ 0.0277,  0.0177, -0.0007,  ...,  0.0419, -0.0116,  0.0184],\n",
      "        [-0.0528, -0.0058,  0.0350,  ..., -0.0061, -0.0236, -0.0245]],\n",
      "       requires_grad=True) \n",
      " bias:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "preds:\n",
      " tensor([-3.1698, -1.8879, -2.2151, -2.3113, -2.5935, -2.6347, -2.4085, -2.0691,\n",
      "        -1.9722, -2.3389], grad_fn=<SelectBackward0>)\n",
      " preds.shape:\n",
      " torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# We are initializing the weights here with Xavier initialisation (by multiplying with 1/sqrt(n))\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "print(f\"weights: \\n {weights} \\n bias:\\n {bias}\")\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)  # @ stands for the matrix multiplication operation.\n",
    "\n",
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(f\"preds:\\n {preds[0]}\\n preds.shape:\\n {preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "837f2db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T02:57:55.401913Z",
     "start_time": "2022-08-02T02:57:55.394758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3506, grad_fn=<NegBackward0>)\n",
      "accurancy: 0.046875\n"
     ]
    }
   ],
   "source": [
    "def nll(input, target):\n",
    "    \"\"\"implement negative log-likelihood to use as the loss function \"\"\"\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll\n",
    "\n",
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "print(f\"accurancy: {accuracy(preds, yb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4383c0",
   "metadata": {},
   "source": [
    "### use torch.nn.functional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd0e6288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T03:28:28.668261Z",
     "start_time": "2022-08-02T03:28:28.651871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3506, grad_fn=<NllLossBackward0>) tensor(0.0469)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias\n",
    "\n",
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9876f6",
   "metadata": {},
   "source": [
    "### refactor using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0420215c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T03:32:54.703523Z",
     "start_time": "2022-08-02T03:32:54.693971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3809, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Logistic, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "\n",
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610d20c",
   "metadata": {},
   "source": [
    "### refactor using nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3032f1f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T03:40:06.657153Z",
     "start_time": "2022-08-02T03:40:06.648999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist_Logistic(\n",
      "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "tensor(2.3352, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Logistic,self).__init__()\n",
    "        self.fc1 = nn.Linear(784,10)\n",
    "    def forward(self, xb):\n",
    "        return self.fc1(xb)\n",
    "model = Mnist_Logistic()\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(model(xb), yb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9089170",
   "metadata": {},
   "source": [
    "### refactor using optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28b15649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T05:28:34.486641Z",
     "start_time": "2022-08-02T05:28:34.479687Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807bc87",
   "metadata": {},
   "source": [
    "### refactor using dataset & dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3850f7c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T05:40:18.425155Z",
     "start_time": "2022-08-02T05:40:16.972674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.6879)\n",
      "1 tensor(1.3220)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_ds, batch_size=bs,shuffle=True)\n",
    "\n",
    "\n",
    "        \n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2) \n",
    "\n",
    "\n",
    "epochs=2\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred= model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "        \n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3fc42",
   "metadata": {},
   "source": [
    "### switch to CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0d32fa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T05:41:13.530512Z",
     "start_time": "2022-08-02T05:41:13.522356Z"
    }
   },
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eec53a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T05:44:23.335555Z",
     "start_time": "2022-08-02T05:44:23.308401Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() takes 0 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     14\u001b[0m     Lambda(preprocess),\n\u001b[1;32m     15\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() takes 0 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e428f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
